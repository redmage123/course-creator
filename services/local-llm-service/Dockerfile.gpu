# Local LLM Service Dockerfile with GPU Support
# Uses NVIDIA CUDA base image for GPU-accelerated inference

FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    DEBIAN_FRONTEND=noninteractive \
    PYTHONPATH=/app \
    LOCAL_LLM_PORT=8015 \
    LOG_LEVEL=INFO \
    OLLAMA_GPU=true \
    CUDA_VISIBLE_DEVICES=0

WORKDIR /app

# Install system dependencies
RUN apt-get update && \
    apt-get install -y \
    python3 \
    python3-pip \
    curl \
    ca-certificates \
    && apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Create app user
RUN useradd -m -u 1000 appuser && \
    mkdir -p /var/log/course-creator && \
    chown -R appuser:appuser /var/log/course-creator

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .
RUN chown -R appuser:appuser /app

USER appuser

EXPOSE 8015

# Start Ollama in background, pull model, then start service
CMD bash -c "ollama serve & \
    sleep 10 && \
    ollama pull llama3.1:8b-instruct-q4_K_M && \
    python3 main.py"
